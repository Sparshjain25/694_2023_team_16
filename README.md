# Introduction
This project aims to create a Twitter search engine that allows users to search for tweets based on specific keywords, hashtags, and usernames. The search engine will utilize a locally stored dataset in the form of JSON objects, which will require preprocessing to clean, organize, and format the data into a searchable format. The search engine will perform natural language processing techniques to filter and rank the tweets based on their relevance to the user's search query. The project will also incorporate a user interface that allows users to input their search queries and browse through the search results. The proposed search engine could be useful for researchers, marketers, journalists, and anyone who wants to search through a specific dataset of tweets for information on a particular topic or trend. The project will require significant computing resources and expertise in data preprocessing and search engine development.

# Search Engine Design
Search Engine is designed using Flask and HTML.
## 5.1 Searches
This Twitter Search Engine supports three types of searches. The search field allows for a minimum of four characters. If the search begins with ’@’ it will look for all users. If the search starts with ’#’ it will look for all hashtags. The tweets are explored if the aforementioned characters are not preceded in the search query. Following the search, the time range for tweets and hashtags is displayed. There are three types of time ranges added: "last day", "last week", and "last month". These metrics are calculated from the most recent tweet present in the collections.
## 5.2 User Search
On searching for users, all the users displayed are retrieved using exact and partial matches. Exact matches are given more priority than partial matches. Firstly, all the account names and usernames which exactly match the search string are retrieved in that order. Secondly, all the account names and usernames that start with the search string are retrieved in that order. Lastly, all the account names and usernames that contain the search string are retrieved in that order. All these results are stored in a set to handle duplicates. All exact and partial match search queries are ordered by verification status, followed by the number of followers in descending order.

## 5.3 Hashtag Search
On searching for hashtags, the search string is searched in the tweets collection. The tweet is retrieved if the searched string is found in the hashtags array of tweets object. The ordering of these tweets is based on the "priority" calculated in Fig 2(b). Additionally, the results can be filtered by selecting time range options as mentioned before. This feature is implemented using the "fmt_time" field.

## 5.4 Tweets Search
On searching for tweets, the keyword is matched in the tweets collection in the "tweet" field. Firstly, the exact matches are found and ordered in decreasing the order of "priority". Now, the search string is matched partially. If the search string contains more than one word, it is broken into individual words and then joined by the regex function to discover partial matches of that string in that sequence. The relative distance between the terms in the searched string determines the order of the matched tweets. Furthermore, tweets with the same relative distances are sorted by decreasing order of "priority". The tweets page has a similar time range feature as mentioned in Hashtag Search(5.3).

## 5.5 Drill-Downs
To begin, the tweets hyperlink is supplied on the user page, which outputs the number of tweets done by the user, which are organized in decreasing order of the "fmt_time" parameter, with the most recent displayed on top. Following that, tweets can be filtered by time ranges. The tweet page also includes a link to the Quoted tweet, which is displayed as "MAIN ID" if it exists. The quoted tweet is found by searching for its id in the tweets collection using the "post_id" indexing. The tweet page also has a "retweets" hyperlink, which displays all of the user’s "Account_names" who have retweeted that tweet in chronological order, with the most recent user at the top. The "post_id" of the tweet is searched for in the retweets collection using "main_tweet_id" indexing. On clicking "Account_names", all the details of the user are displayed. For this, the "user_id" present in the retweets collection is searched in the relational "user" table. In the Search Engine, four metrics are displayed. The first is the "Top 20 Handles" section, which shows the top 20 accounts in decreasing order of number of followers. The second is the "Top 20 Celebs" section, which displays the top 20
accounts ranked first by verified status and then by number of followers. The third option is the "Top 20 Active Users" option, which displays the top 20 accounts in decreasing order of the number of tweets made by the accounts. The final option is "Top 20 Tweets," which displays the top 20 tweets in the decreasing order of "priority".

# Caching
A Python dictionary and a doubly linked list were used to construct an LRU cache. It may have been investigated to use hashmaps or another data structure for this approach. It would have been possible to investigate and contrast the many different caching algorithms and their eviction procedures. Javascript and Bootstrap might have been used to enhance the user interface. During the course of this project, MySQL and MongoDB were thoroughly investigated. Learned about relational databases and non-relational data stores, as well as the distinctions and applications for each. We now understand the Twitter JSON structure and how to deal with such huge and complex data. This project assisted us in developing methods for the appropriate arrangement of outputs.  It aided us in learning Flask and HTML, as well as the integration of databases and data repositories with User Interface. One of the most intriguing aspects of the project was implementing a cache for quick retrieval and comparing fetch times with and without the cache.

# Conclusion
A Python dictionary and a doubly linked list were used to construct an LRU cache. It may have been investigated to use hashmaps or another data structure for this approach. It would have been possible to investigate and contrast the many different caching algorithms and their eviction procedures. Javascript and Bootstrap might have been used to enhance the user interface. During the course of this project, MySQL and MongoDB were thoroughly investigated. Learned about relational databases and non-relational data stores, as well as the distinctions and applications for each. We now understand the Twitter JSON structure and how to deal with such huge and complex data. This project assisted us in developing methods for the appropriate arrangement of outputs.  It aided us in learning Flask and HTML, as well as the integration of databases and data repositories with User Interface. One of the most intriguing aspects of the project was implementing a cache for quick retrieval and comparing fetch times with and without the cache.
